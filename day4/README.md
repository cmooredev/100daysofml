# Logistic Regression
Today, we went over logistic regression.

https://web.stanford.edu/~jurafsky/slp3/5.pdf

This chapter goes over basic concepts like the sigmoid function which helps us turn our values into probabilities.  
The text also goes over features and how their weights indicate how important the features are for certain decisions. 
The decision boundary is used along with the sigmoid function when classifying. For example, if the decision boundary was 0.5, anything below
would become 0, and anything above would become a 1.  Usually 0 indicates false and 1 indicates true, as in true that this values belongs to this class. 
The text is focused on NLP and most of the examples are shown through this perspective. 
